{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Cuda Available: True\n",
      "Device Count: 1\n",
      "Device Name: NVIDIA GeForce GTX 965M\n"
     ]
    }
   ],
   "source": [
    "# Test Cuda Device\n",
    "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device Count: {torch.cuda.device_count()}\")\n",
    "print(f\"Device Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Flan-T5 \"small\"; Example Code from [website](https://huggingface.co/docs/transformers/en/model_doc/flan-t5) + `Cuda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pour a cup of bolognese into a large bowl and add the pasta to']\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Flan-T5 \"large\" testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-large\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def correct_text(text):\n",
    "    prompt = f\"Correct the grammar and meaning of this sentence and return only the corrected version: {text}\"\n",
    "    # inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # outputs = model.generate(**inputs, max_length=512)\n",
    "    outputs = model.generate(input_ids, max_length=200)\n",
    "    corrected_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect Text: She go to the store and buyed a apple for her brother.\n",
      "Corrected Text: She go to the store and buyed a apple for her brother.\n",
      "--------------------------------------------------\n",
      "Incorrect Text: The cat was barking loudly at the tree while the dog was climbing it.\n",
      "Corrected Text: The cat was barking loudly at the tree while the dog was climbing it.\n",
      "--------------------------------------------------\n",
      "Incorrect Text: Yesterday, I movie with my friends.\n",
      "Corrected Text: Yesterday, I movie with my friends.\n",
      "--------------------------------------------------\n",
      "Incorrect Text: Beautiful very is this place.\n",
      "Corrected Text: Beautiful very is this place.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "texts = [\n",
    "    \"She go to the store and buyed a apple for her brother.\",  # Grammar Mistake\n",
    "    \"The cat was barking loudly at the tree while the dog was climbing it.\",  # Semantic Mistake\n",
    "    \"Yesterday, I movie with my friends.\",  # Missing Word\n",
    "    \"Beautiful very is this place.\",   # Word Order Error\n",
    "]\n",
    "\n",
    "# Write Corrected Word\n",
    "for text in texts:\n",
    "    print(\"Incorrect Text:\", text)\n",
    "    print(\"Corrected Text:\", correct_text(text))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I tested the `large` version of Google's `Flan-T5` model to see if it could fix grammatical errors and incorrect meanings in sentences, but it didn't work. \n",
    "- The `xl` and `xxl` versions may work, but after finding out that they don't work as I wanted, there's no need to test them any further. Let's move on to the new models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
